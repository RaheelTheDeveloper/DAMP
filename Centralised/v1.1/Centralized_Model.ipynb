{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Centralized_Model.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oosbIudycJ9U"
      },
      "source": [
        "# Centralized Model Competition\n",
        "Instructions:\n",
        "1. Run the file `Centralized_Model.ipynb` in Google Colab environment\n",
        "2. Load the given dataset in the following directory `/content/`\n",
        "3. Create a folder `data` in the following directory `/content/` to store the .csv files for the predictions made by the model for each stations.  \n",
        "4. After the execution is completed, download `data.zip` which contains the prediction values for all the stations.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pqE9YpJgpLOk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c8487334-0232-4f47-92c4-82d8d227f2e7"
      },
      "source": [
        "# Import libraries\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import missingno as msno\n",
        "import re\n",
        "import os\n",
        "import datetime\n",
        "import IPython\n",
        "import IPython.display\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "mpl.rcParams['figure.figsize'] = (8, 6)\n",
        "mpl.rcParams['axes.grid'] = False\n",
        "\n",
        "OUT_STEPS = 24\n",
        "MAX_EPOCHS = 20\n",
        "num_features = 4\n",
        "n = 33293  # 80% of index\n",
        "\n",
        "# Helper Functions\n",
        "# Function to create Dataframe for each station\n",
        "def create_df(PATH):\n",
        "    # Loading the data\n",
        "    df = pd.read_csv(PATH,sep=';',skiprows=12)\n",
        "    df = df.drop(columns=['Slut'])\n",
        "    column_indices = {i: name for i, name in enumerate(df.columns)}\n",
        "    # Renaming column names\n",
        "    for i in range(0,len(column_indices)):\n",
        "        column = column_indices[i]\n",
        "        if column.startswith('Black Carbon'):\n",
        "            df.rename(columns = lambda x: re.sub('Black Carbon.*','Black Carbon',x), inplace = True)\n",
        "        if column.startswith('CO'):\n",
        "            df.rename(columns = lambda x: re.sub('CO.*','CO',x), inplace = True)\n",
        "        if column.startswith('O3'):\n",
        "            df.rename(columns = lambda x: re.sub('O3.*','O3',x), inplace = True)\n",
        "        if column.startswith('NO2'):\n",
        "            df.rename(columns = lambda x: re.sub('NO2.*','NO2',x), inplace = True)\n",
        "        if column.startswith('NOX as NO2'):\n",
        "            df.rename(columns = lambda x: re.sub('NOX as NO2.*','NOX as NO2',x), inplace = True)\n",
        "        if column.startswith('PM10'):\n",
        "            df.rename(columns = lambda x: re.sub('PM10.*','PM10',x), inplace = True)\n",
        "        if column.startswith('PM2.5'):\n",
        "            df.rename(columns = lambda x: re.sub('PM2.5.*','PM2.5',x), inplace = True)\n",
        "    \n",
        "    df = df.set_index('Start')\n",
        "    df1 = pd.DataFrame()\n",
        "    # Adding Air Pressure from Stockholm Station\n",
        "    pressure = pd.read_csv('/content/stockholm-airpressure.csv',sep=';',skiprows= 8)\n",
        "    pressure = pressure.drop(columns=['Tidsutsnitt:','Unnamed: 4','Kvalitet'])\n",
        "    pressure.rename(columns = lambda x: re.sub('Lufttryck.*','Air Pressure',x), inplace = True)\n",
        "    pressure.rename(columns = lambda x: re.sub('Tid.*','Time',x), inplace = True) \n",
        "    pressure.rename(columns = lambda x: re.sub('Datum.*','Date',x), inplace = True) \n",
        "    pressure['Start']= pd.to_datetime(pressure['Date'] + ' ' + pressure['Time'])\n",
        "    pressure = pressure.drop(columns=['Date','Time'])\n",
        "    pressure = pressure[(pressure['Start']>= \"2015-01-01 00:00:00\")]\n",
        "    pressure = pressure[(pressure['Start']<=\"2019-12-31 23:00:00\")]\n",
        "    pressure = pressure.set_index('Start')\n",
        "    df1 = df1.merge(pressure, left_index=True, right_index=True ,how='outer')\n",
        "    \n",
        "    # Adding Air Temperature from Stockholm Station\n",
        "    temp = pd.read_csv('/content/stockholm-airtemp.csv',sep=';',skiprows= 8)\n",
        "    temp = temp.drop(columns=['Tidsutsnitt:','Unnamed: 4','Kvalitet']) \n",
        "    temp.rename(columns = lambda x: re.sub('Lufttemperatur.*','Air Temperature',x), inplace = True)\n",
        "    temp.rename(columns = lambda x: re.sub('Tid.*','Time',x), inplace = True) \n",
        "    temp.rename(columns = lambda x: re.sub('Datum.*','Date',x), inplace = True) \n",
        "    temp['Start']= pd.to_datetime(temp['Date'] + ' ' + temp['Time'])\n",
        "    temp = temp.drop(columns=['Date','Time'])\n",
        "    temp = temp[(temp['Start']>=\"2015-01-01 00:00:00\")]\n",
        "    temp = temp[(temp['Start']<=\"2019-12-31 23:00:00\")]\n",
        "    temp = temp.set_index('Start')\n",
        "    df1 = df1.merge(temp, left_index=True, right_index=True ,how='outer')\n",
        "    \n",
        "    # Adding Relative Humidity data from Stockholm Station\n",
        "    humidity = pd.read_csv('/content/stockholm-humidity.csv',sep=';',skiprows= 8)\n",
        "    humidity = humidity.drop(columns=['Tidsutsnitt:','Unnamed: 4','Kvalitet']) \n",
        "    humidity.rename(columns = lambda x: re.sub('Relativ Luftfuktighet.*','Humidity',x), inplace = True)\n",
        "    humidity.rename(columns = lambda x: re.sub('Tid.*','Time',x), inplace = True) \n",
        "    humidity.rename(columns = lambda x: re.sub('Datum.*','Date',x), inplace = True) \n",
        "    humidity['Start']= pd.to_datetime(humidity['Date'] + ' ' + humidity['Time'])\n",
        "    humidity = humidity.drop(columns=['Date','Time'])\n",
        "    humidity = humidity[(humidity['Start']>= \"2015-01-01 00:00:00\")]\n",
        "    humidity = humidity[(humidity['Start']<=\"2019-12-31 23:00:00\")]\n",
        "    humidity = humidity.set_index('Start')\n",
        "    df1 = df1.merge(humidity, left_index=True, right_index=True ,how='outer')\n",
        "    cols = ['Air Pressure','Air Temperature','Humidity']\n",
        "    df1.loc[:,cols] = df1.loc[:,cols].ffill()\n",
        "    df1.loc[:,cols] = df1.loc[:,cols].bfill()\n",
        "    df = df.merge(df1, left_index=True, right_index=True, how='outer')\n",
        "    df.reset_index(drop=False, inplace= True)\n",
        "    \n",
        "    # Converting datatype of feature 'Start' to datetime\n",
        "    df['Start'] = pd.to_datetime(df['Start'])\n",
        "    df['Year'] = df['Start'].dt.year\n",
        "    df['Month'] = df['Start'].dt.month\n",
        "    df['Week'] = df['Start'].dt.week\n",
        "    df['DayOfWeek'] = df['Start'].dt.day_name()\n",
        "    df = pd.concat([df,pd.get_dummies(df['DayOfWeek'])],axis=1)\n",
        "    weekend={'Sunday':1,'Monday':0,'Tuesday':0,'Wednesday':0,'Thursday':0,'Friday':0,'Saturday':1}\n",
        "    weekday={'Sunday':0,'Monday':1,'Tuesday':1,'Wednesday':1,'Thursday':1,'Friday':1,'Saturday':0}\n",
        "    season={1:'Winter', 2:'Winter', 3:'Spring', 4:'Spring', 5:'Spring', 6:'Summer', 7:'Summer', 8:'Summer', 9:'Fall', 10:'Fall', 11:'Fall', 12:'Winter'} \n",
        "    df['Weekend']= df['DayOfWeek'].map(weekend)\n",
        "    df['Weekday']= df['DayOfWeek'].map(weekday)\n",
        "    df['Season']= df['Month'].map(season)\n",
        "    df = pd.concat([df,pd.get_dummies(df['Season'])],axis=1)\n",
        "    df = df.drop(columns=['DayOfWeek','Year','Month','Week','Season','Air Temperature'])\n",
        "    return df\n",
        "\n",
        "# Function to display the Missing Values\n",
        "def missingstats():  \n",
        "  print('Station 1 Missing Values Stats:')\n",
        "  print('----------------------------------------------')\n",
        "  print('\\nNo of Zeros Entries:\\n',(station1.select_dtypes(include=['float64']) == 0).astype(int).sum(axis=0))\n",
        "  print('\\nNo of Negative Entries:\\n',(station1.select_dtypes(include=['float64']) < 0).astype(int).sum(axis=0))\n",
        "  print('\\nNo of Null Entries:\\n',(station1.select_dtypes(include=['float64']).isnull()).astype(int).sum(axis=0))\n",
        "  print('\\n\\n')\n",
        "\n",
        "  print('Station 2 Missing Values Stats:')\n",
        "  print('----------------------------------------------')\n",
        "  print('\\nNo of Zeros Entries:\\n',(station2.select_dtypes(include=['float64']) == 0).astype(int).sum(axis=0))\n",
        "  print('\\nNo of Negative Entries:\\n',(station2.select_dtypes(include=['float64']) < 0).astype(int).sum(axis=0))\n",
        "  print('\\nNo of Null Entries:\\n',(station2.select_dtypes(include=['float64']).isnull()).astype(int).sum(axis=0))\n",
        "  print('\\n\\n')\n",
        "\n",
        "  print('Station 3 Missing Values Stats:')\n",
        "  print('----------------------------------------------')\n",
        "  print('\\nNo of Zeros Entries:\\n',(station3.select_dtypes(include=['float64']) == 0).astype(int).sum(axis=0))\n",
        "  print('\\nNo of Negative Entries:\\n',(station3.select_dtypes(include=['float64']) < 0).astype(int).sum(axis=0))\n",
        "  print('\\nNo of Null Entries:\\n',(station3.select_dtypes(include=['float64']).isnull()).astype(int).sum(axis=0))\n",
        "  print('\\n\\n')\n",
        "\n",
        "  print('Station 4 Missing Values Stats:')\n",
        "  print('----------------------------------------------')\n",
        "  print('\\nNo of Zeros Entries:\\n',(station4.select_dtypes(include=['float64']) == 0).astype(int).sum(axis=0))\n",
        "  print('\\nNo of Negative Entries:\\n',(station4.select_dtypes(include=['float64']) < 0).astype(int).sum(axis=0))\n",
        "  print('\\nNo of Null Entries:\\n',(station4.select_dtypes(include=['float64']).isnull()).astype(int).sum(axis=0))\n",
        "  print('\\n\\n')\n",
        "  return None\n",
        "\n",
        "\n",
        "# Function to calculate SMAPE\n",
        "def smape(actual, predicted):\n",
        "   dividend= np.abs(np.array(actual) - np.array(predicted))\n",
        "   denominator = np.array(actual) + np.array(predicted)\n",
        "   return 2 * np.mean(np.divide(dividend, denominator, out=np.zeros_like(dividend), where=denominator!=0, casting='unsafe'))\n",
        "\n",
        "PATH1 = '/content/shair-8779-1-6-3.csv'\n",
        "PATH2 = '/content/shair-8780-1-6-3.csv'\n",
        "PATH3 = '/content/shair-8781-1-6-1.csv'\n",
        "PATH4 = '/content/shair-18644-1-6-3.csv'\n",
        "station1 = create_df(PATH1)\n",
        "station2 = create_df(PATH2)\n",
        "station3 = create_df(PATH3)\n",
        "station4 = create_df(PATH4)\n",
        "smape_values = pd.DataFrame(columns=['Station','NO2','NOX as NO2','PM10','PM2.5','Average'])\n",
        "r2_scores = pd.DataFrame(columns=['Station','NO2','NOX as NO2','PM10','PM2.5'])\n",
        "\n",
        "\n",
        "station1 = station1.set_index('Start')\n",
        "station1[station1 < 0] = 0\n",
        "station1.reset_index(drop=False, inplace= True)\n",
        "station1 = station1.interpolate(method ='linear', limit_direction ='forward')\n",
        "\n",
        "station2 = station2.set_index('Start')\n",
        "station2[station2 < 0] = 0\n",
        "station2.reset_index(drop=False, inplace= True)\n",
        "station2 = station2.interpolate(method ='linear', limit_direction ='forward')\n",
        "\n",
        "station3 = station3.set_index('Start')\n",
        "station3[station3 < 0] = 0\n",
        "station3.reset_index(drop=False, inplace= True)\n",
        "station3 = station3.interpolate(method ='linear', limit_direction ='forward')\n",
        "\n",
        "station4 = station4.set_index('Start')\n",
        "station4[station4 < 0] = 0\n",
        "station4.reset_index(drop=False, inplace= True)\n",
        "station4 = station4.interpolate(method ='linear', limit_direction ='forward')\n",
        "#missingstats()\n",
        "\n",
        "for count in range(1,5):\n",
        "  print('\\nStation-',count)\n",
        "  print('----------------------------------------------')\n",
        "  if count == 1:\n",
        "  #Split for station 1\n",
        "    index = station1[station1['Start']=='2019-09-30 00:00:00'].index.values.astype(int)[0]\n",
        "    date_time = pd.to_datetime(station1.pop('Start'), format='%d.%m.%Y %H:%M:%S')\n",
        "    column_indices = {name: i for i, name in enumerate(station1.columns)}\n",
        "    train_df = station1.iloc[:n,:]\n",
        "    val_df = station1.iloc[n:index,:]\n",
        "    test_df = station1.iloc[index:,:]\n",
        "    station = 8779\n",
        "\n",
        "  if count == 2:\n",
        "  #Split for station 2\n",
        "    index = station2[station2['Start']=='2019-09-30 00:00:00'].index.values.astype(int)[0]\n",
        "    date_time = pd.to_datetime(station2.pop('Start'), format='%d.%m.%Y %H:%M:%S')\n",
        "    column_indices = {name: i for i, name in enumerate(station2.columns)}\n",
        "    train_df = station2.iloc[:n,:]\n",
        "    val_df = station2.iloc[n:index,:]\n",
        "    test_df = station2.iloc[index:,:]\n",
        "    station = 8780\n",
        "\n",
        "  if count == 3:\n",
        "    #Split for station 3\n",
        "    index = station3[station3['Start']=='2019-09-30 00:00:00'].index.values.astype(int)[0]\n",
        "    date_time = pd.to_datetime(station3.pop('Start'), format='%d.%m.%Y %H:%M:%S')\n",
        "    column_indices = {name: i for i, name in enumerate(station3.columns)}\n",
        "    train_df = station3.iloc[:n,:]\n",
        "    val_df = station3.iloc[n:index,:]\n",
        "    test_df = station3.iloc[index:,:]\n",
        "    station = 8781\n",
        "\n",
        "  #Split for station 4\n",
        "  if count == 4:\n",
        "    index = station4[station4['Start']=='2019-09-30 00:00:00'].index.values.astype(int)[0]\n",
        "    date_time = pd.to_datetime(station4.pop('Start'), format='%d.%m.%Y %H:%M:%S')\n",
        "    column_indices = {name: i for i, name in enumerate(station4.columns)}\n",
        "    train_df = station4.iloc[:n,:]\n",
        "    val_df = station4.iloc[n:index,:]\n",
        "    test_df = station4.iloc[index:,:]\n",
        "    station = 18644\n",
        "  \n",
        "  # Normalization\n",
        "  train_mean = train_df.mean()\n",
        "  train_std = train_df.std()\n",
        "  train_df = (train_df - train_mean) / train_std\n",
        "  val_df = (val_df - train_mean) / train_std\n",
        "  test_df = (test_df - train_mean) / train_std\n",
        "  if count == 1:\n",
        "    df_std = (station1 - train_mean) / train_std\n",
        "\n",
        "  if count == 2:\n",
        "    df_std = (station2 - train_mean) / train_std\n",
        "  \n",
        "  if count == 3:\n",
        "    df_std = (station3 - train_mean) / train_std\n",
        "  \n",
        "  if count == 4:\n",
        "    df_std = (station4 - train_mean) / train_std\n",
        "\n",
        "  #df_std = df_std.melt(var_name='Column', value_name='Normalized')\n",
        "  #plt.figure(figsize=(12, 6))\n",
        "  #ax = sns.violinplot(x='Column', y='Normalized', data=df_std)\n",
        "  #_ = ax.set_xticklabels(station4.keys(), rotation=90)\n",
        "\n",
        "  # Class to define a window\n",
        "  class WindowGenerator():\n",
        "    def __init__(self, input_width, label_width, shift,\n",
        "                train_df=train_df, val_df=val_df, test_df=test_df,\n",
        "                label_columns=None):\n",
        "\n",
        "      self.train_df = train_df\n",
        "      self.val_df = val_df\n",
        "      self.test_df = test_df\n",
        "\n",
        "      self.label_columns = label_columns\n",
        "      if label_columns is not None:\n",
        "        self.label_columns_indices = {name: i for i, name in\n",
        "                                      enumerate(label_columns)}\n",
        "      self.column_indices = {name: i for i, name in\n",
        "                            enumerate(train_df.columns)}\n",
        "\n",
        "      self.input_width = input_width\n",
        "      self.label_width = label_width\n",
        "      self.shift = shift\n",
        "\n",
        "      self.total_window_size = input_width + shift\n",
        "\n",
        "      self.input_slice = slice(0, input_width)\n",
        "      self.input_indices = np.arange(self.total_window_size)[self.input_slice]\n",
        "\n",
        "      self.label_start = self.total_window_size - self.label_width\n",
        "      self.labels_slice = slice(self.label_start, None)\n",
        "      self.label_indices = np.arange(self.total_window_size)[self.labels_slice]\n",
        "\n",
        "    def __repr__(self):\n",
        "      return '\\n'.join([\n",
        "          f'Total window size: {self.total_window_size}',\n",
        "          f'Input indices: {self.input_indices}',\n",
        "          f'Label indices: {self.label_indices}',\n",
        "          f'Label column name(s): {self.label_columns}'])\n",
        "  # Function to Split window\n",
        "  def split_window(self, features):\n",
        "    inputs = features[:, self.input_slice, :]\n",
        "    labels = features[:, self.labels_slice, :]\n",
        "    if self.label_columns is not None:\n",
        "      labels = tf.stack(\n",
        "          [labels[:, :, self.column_indices[name]] for name in self.label_columns],\n",
        "          axis=-1)\n",
        "    inputs.set_shape([None, self.input_width, None])\n",
        "    labels.set_shape([None, self.label_width, None])\n",
        "\n",
        "    return inputs, labels\n",
        "\n",
        "  # Function to Plot\n",
        "  def plot(self, model=None, plot_col='PM10', max_subplots=3):\n",
        "    inputs, labels = self.example\n",
        "    plt.figure(figsize=(12, 8))\n",
        "    plot_col_index = self.column_indices[plot_col]\n",
        "    max_n = min(max_subplots, len(inputs))\n",
        "    for n in range(max_n):\n",
        "      plt.subplot(3, 1, n+1)\n",
        "      plt.ylabel(f'{plot_col} [normed]')\n",
        "      plt.plot(self.input_indices, inputs[n, :, plot_col_index],\n",
        "              label='Inputs', marker='.', zorder=-10)\n",
        "\n",
        "      if self.label_columns:\n",
        "        label_col_index = self.label_columns_indices.get(plot_col, None)\n",
        "      else:\n",
        "        label_col_index = plot_col_index\n",
        "\n",
        "      if label_col_index is None:\n",
        "        continue\n",
        "\n",
        "      plt.scatter(self.label_indices, labels[n, :, label_col_index],\n",
        "                  edgecolors='k', label='Labels', c='#2ca02c', s=64)\n",
        "      if model is not None:\n",
        "        predictions = model(inputs)\n",
        "        plt.scatter(self.label_indices, predictions[n, :, label_col_index], \n",
        "                    marker='X', edgecolors='k', label='Predictions',\n",
        "                    c='#ff7f0e', s=64)\n",
        "      if n == 0:\n",
        "        plt.legend()\n",
        "    plt.xlabel('Time [h]')\n",
        "\n",
        "  # Function to Make Dataset\n",
        "  def make_dataset(self, data):\n",
        "    data = np.array(data, dtype=np.float32)\n",
        "    ds = tf.keras.preprocessing.timeseries_dataset_from_array(\n",
        "        data=data,\n",
        "        targets=None,\n",
        "        sequence_length=self.total_window_size,\n",
        "        sequence_stride=1,\n",
        "        shuffle=True,\n",
        "        batch_size=32,)\n",
        "\n",
        "    ds = ds.map(self.split_window)\n",
        "\n",
        "    return ds\n",
        "\n",
        "  @property\n",
        "  def train(self):\n",
        "    return self.make_dataset(self.train_df)\n",
        "\n",
        "  @property\n",
        "  def val(self):\n",
        "    return self.make_dataset(self.val_df)\n",
        "\n",
        "  @property\n",
        "  def test(self):\n",
        "    return self.make_dataset(self.test_df)\n",
        "\n",
        "  @property\n",
        "  def example(self):\n",
        "    \"\"\"Get and cache an example batch of `inputs, labels` for plotting.\"\"\"\n",
        "    result = getattr(self, '_example', None)\n",
        "    if result is None:\n",
        "      result = next(iter(self.train))\n",
        "      self._example = result\n",
        "    return result\n",
        "\n",
        "  # Function to Compile the model \n",
        "  def compile_and_fit(model, window, patience=2):\n",
        "      \n",
        "      early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss',\n",
        "                                                        patience=patience,\n",
        "                                                        mode='min')\n",
        "      def smape(y_true, y_pred):\n",
        "            y_true = y_true * train_std[:num_features] + train_mean[:num_features]\n",
        "            y_pred = y_pred * train_std[:num_features] + train_mean[:num_features]\n",
        "            return tf.reduce_mean(2 * tf.abs(y_true - y_pred)\n",
        "                                  / (tf.abs(y_pred) + tf.abs(y_true)), axis=-1)\n",
        "      model.compile(loss=tf.losses.MeanSquaredError(),\n",
        "                    optimizer=tf.optimizers.Adam(),\n",
        "                    metrics=[smape,tf.metrics.MeanAbsoluteError()])\n",
        "      history = model.fit(window.train, epochs=MAX_EPOCHS,\n",
        "                        validation_data=window.val,\n",
        "                        callbacks=[early_stopping])\n",
        "      return history\n",
        "\n",
        "  lstm_model = tf.keras.Sequential([\n",
        "      tf.keras.layers.LSTM(32, return_sequences=False),\n",
        "      tf.keras.layers.Dense(OUT_STEPS*num_features,\n",
        "                            kernel_initializer=tf.initializers.zeros),\n",
        "      tf.keras.layers.Reshape([OUT_STEPS, num_features])\n",
        "    ])\n",
        "\n",
        "  WindowGenerator.split_window = split_window\n",
        "  WindowGenerator.plot = plot\n",
        "  WindowGenerator.make_dataset = make_dataset\n",
        "  WindowGenerator.train = train\n",
        "  WindowGenerator.val = val\n",
        "  WindowGenerator.test = test\n",
        "  WindowGenerator.example = example\n",
        "  \n",
        "  multi_window = WindowGenerator(input_width=24,\n",
        "                                label_width=OUT_STEPS, \n",
        "                                shift=24,label_columns=['NO2','NOX as NO2','PM10','PM2.5'])\n",
        "\n",
        "  history = compile_and_fit(lstm_model,multi_window)\n",
        "  multi_val_performance = {}\n",
        "  multi_performance = {}\n",
        "  multi_val_performance['LSTM'] = lstm_model.evaluate(multi_window.val)\n",
        "  multi_performance['LSTM'] = lstm_model.evaluate(multi_window.test, verbose=1)\n",
        "  #multi_window.plot(multi_lstm_model)\n",
        "\n",
        "  predict = pd.DataFrame(columns=['NO2','NOX as NO2','PM10','PM2.5'])\n",
        "  idx= 0\n",
        "  for j in range(0,92):\n",
        "    test = test_df.iloc[idx:idx+48,:]\n",
        "    predict_window = WindowGenerator(input_width=24,\n",
        "                                label_width=24, \n",
        "                                test_df = test,\n",
        "                                shift=24,label_columns=['NO2','NOX as NO2','PM10','PM2.5'])\n",
        "    values = lstm_model.predict(predict_window.test)\n",
        "    for i in range(0,24):\n",
        "        dictionary = {'NO2':values[0][i][0], 'NOX as NO2':values[0][i][1],'PM10':values[0][i][2], 'PM2.5':values[0][i][3]}\n",
        "        predict = predict.append(dictionary,ignore_index=True)\n",
        "    idx=idx+24\n",
        "  predict= (predict*train_std)+train_mean\n",
        "  predict.drop(columns=['Air Pressure', 'Fall', 'Friday', 'Humidity', 'Monday',\n",
        "          'Saturday', 'Spring', 'Summer', 'Sunday',\n",
        "          'Thursday', 'Tuesday', 'Wednesday', 'Weekday', 'Weekend', 'Winter'])\n",
        "  predict['Start'] = pd.date_range(start='2019-10-01 00:00:00', periods=len(predict), freq='H')\n",
        "  predict = predict[['Start','NO2','NOX as NO2','PM10','PM2.5']]\n",
        "  predict.to_csv(r'/content/data/'+str(station)+'.csv', index = False)\n",
        "  \n",
        "  # Evaluating Actual vs Predicted values\n",
        "  if count == 1:\n",
        "    actual=station1.iloc[41616:,:]\n",
        "  \n",
        "  if count == 2:\n",
        "    actual=station2.iloc[41616:,:]\n",
        "\n",
        "  if count == 3:\n",
        "    actual=station3.iloc[41616:,:]\n",
        "\n",
        "  if count == 4:\n",
        "    actual=station4.iloc[41616:,:]\n",
        "\n",
        "  actual.index = range(len(predict))\n",
        "  no2 = smape(actual['NO2'],predict['NO2'])\n",
        "  no2_r2 = r2_score(actual['NO2'], predict['NO2'])\n",
        "  nox = smape(actual['NOX as NO2'],predict['NOX as NO2'])\n",
        "  nox_r2 = r2_score(actual['NOX as NO2'],predict['NOX as NO2'])\n",
        "  pm2_5 = smape(actual['PM2.5'],predict['PM2.5'])\n",
        "  pm2_5_r2 = r2_score(actual['PM2.5'], predict['PM2.5'])\n",
        "  pm10 = smape(actual['PM10'], predict['PM10'])\n",
        "  pm10_r2 = r2_score(actual['PM10'],predict['PM10']) \n",
        "  dict1 = {'Station':str(station),'NO2':no2,'NOX as NO2':nox,'PM2.5':pm2_5,'PM10':pm10, 'Average':(no2+nox+pm2_5+pm10)/4 }\n",
        "  dict2 = {'Station':str(station),'NO2':no2_r2,'NOX as NO2':nox_r2,'PM2.5':pm2_5_r2,'PM10':pm10_r2}\n",
        "  smape_values = smape_values.append(dict1,ignore_index=True)\n",
        "  r2_scores = r2_scores.append(dict2,ignore_index=True)\n",
        "\n",
        "print('\\nSmape Values for each station:')\n",
        "print(smape_values)\n",
        "print('\\nR2 Scores:')\n",
        "print(r2_scores)\n",
        "!zip -r /content/data.zip /content/data"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py:2822: DtypeWarning: Columns (5) have mixed types.Specify dtype option on import or set low_memory=False.\n",
            "  if self.run_code(code, result):\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:99: FutureWarning: Series.dt.weekofyear and Series.dt.week have been deprecated.  Please use Series.dt.isocalendar().week instead.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Station- 1\n",
            "----------------------------------------------\n",
            "Epoch 1/20\n",
            "1039/1039 [==============================] - 15s 14ms/step - loss: 0.7138 - smape: 0.5464 - mean_absolute_error: 0.5785 - val_loss: 0.6366 - val_smape: 0.5189 - val_mean_absolute_error: 0.5368\n",
            "Epoch 2/20\n",
            "1039/1039 [==============================] - 14s 14ms/step - loss: 0.6037 - smape: 0.5109 - mean_absolute_error: 0.5232 - val_loss: 0.6345 - val_smape: 0.5186 - val_mean_absolute_error: 0.5352\n",
            "Epoch 3/20\n",
            "1039/1039 [==============================] - 14s 14ms/step - loss: 0.5725 - smape: 0.5035 - mean_absolute_error: 0.5088 - val_loss: 0.6440 - val_smape: 0.5213 - val_mean_absolute_error: 0.5397\n",
            "Epoch 4/20\n",
            "1039/1039 [==============================] - 15s 14ms/step - loss: 0.5527 - smape: 0.4992 - mean_absolute_error: 0.5000 - val_loss: 0.6460 - val_smape: 0.5230 - val_mean_absolute_error: 0.5419\n",
            "258/258 [==============================] - 1s 6ms/step - loss: 0.6460 - smape: 0.5230 - mean_absolute_error: 0.5419\n",
            "69/69 [==============================] - 0s 5ms/step - loss: 0.5044 - smape: 0.5508 - mean_absolute_error: 0.5089\n",
            "\n",
            "Station- 2\n",
            "----------------------------------------------\n",
            "Epoch 1/20\n",
            "1039/1039 [==============================] - 15s 14ms/step - loss: 0.6884 - smape: 0.4875 - mean_absolute_error: 0.5773 - val_loss: 0.7044 - val_smape: 0.5132 - val_mean_absolute_error: 0.5650\n",
            "Epoch 2/20\n",
            "1039/1039 [==============================] - 16s 15ms/step - loss: 0.5717 - smape: 0.4484 - mean_absolute_error: 0.5181 - val_loss: 0.6963 - val_smape: 0.5073 - val_mean_absolute_error: 0.5588\n",
            "Epoch 3/20\n",
            "1039/1039 [==============================] - 15s 15ms/step - loss: 0.5379 - smape: 0.4392 - mean_absolute_error: 0.5026 - val_loss: 0.7056 - val_smape: 0.5086 - val_mean_absolute_error: 0.5612\n",
            "Epoch 4/20\n",
            "1039/1039 [==============================] - 15s 15ms/step - loss: 0.5138 - smape: 0.4336 - mean_absolute_error: 0.4925 - val_loss: 0.7234 - val_smape: 0.5094 - val_mean_absolute_error: 0.5651\n",
            "258/258 [==============================] - 2s 6ms/step - loss: 0.7234 - smape: 0.5094 - mean_absolute_error: 0.5651\n",
            "69/69 [==============================] - 0s 5ms/step - loss: 0.4899 - smape: 0.5317 - mean_absolute_error: 0.4949\n",
            "\n",
            "Station- 3\n",
            "----------------------------------------------\n",
            "Epoch 1/20\n",
            "1039/1039 [==============================] - 16s 15ms/step - loss: 0.7411 - smape: 0.4926 - mean_absolute_error: 0.5439 - val_loss: 0.6961 - val_smape: 0.5131 - val_mean_absolute_error: 0.5232\n",
            "Epoch 2/20\n",
            "1039/1039 [==============================] - 15s 15ms/step - loss: 0.6574 - smape: 0.4635 - mean_absolute_error: 0.5064 - val_loss: 0.7015 - val_smape: 0.5074 - val_mean_absolute_error: 0.5195\n",
            "Epoch 3/20\n",
            "1039/1039 [==============================] - 15s 15ms/step - loss: 0.6140 - smape: 0.4534 - mean_absolute_error: 0.4905 - val_loss: 0.7165 - val_smape: 0.5047 - val_mean_absolute_error: 0.5193\n",
            "258/258 [==============================] - 2s 6ms/step - loss: 0.7165 - smape: 0.5047 - mean_absolute_error: 0.5193\n",
            "69/69 [==============================] - 0s 6ms/step - loss: 0.6324 - smape: 0.5419 - mean_absolute_error: 0.5331\n",
            "\n",
            "Station- 4\n",
            "----------------------------------------------\n",
            "Epoch 1/20\n",
            "1039/1039 [==============================] - 16s 15ms/step - loss: 0.6799 - smape: 0.4996 - mean_absolute_error: 0.5677 - val_loss: 0.5582 - val_smape: 0.4859 - val_mean_absolute_error: 0.5169\n",
            "Epoch 2/20\n",
            "1039/1039 [==============================] - 16s 15ms/step - loss: 0.5545 - smape: 0.4637 - mean_absolute_error: 0.5073 - val_loss: 0.5633 - val_smape: 0.4844 - val_mean_absolute_error: 0.5158\n",
            "Epoch 3/20\n",
            "1039/1039 [==============================] - 17s 17ms/step - loss: 0.5177 - smape: 0.4519 - mean_absolute_error: 0.4897 - val_loss: 0.5671 - val_smape: 0.4829 - val_mean_absolute_error: 0.5152\n",
            "258/258 [==============================] - 2s 8ms/step - loss: 0.5671 - smape: 0.4829 - mean_absolute_error: 0.5152\n",
            "69/69 [==============================] - 0s 7ms/step - loss: 0.3793 - smape: 0.4805 - mean_absolute_error: 0.4416\n",
            "\n",
            "Smape Values for each station:\n",
            "  Station       NO2  NOX as NO2      PM10     PM2.5   Average\n",
            "0    8779  0.502877    0.685551  0.537766  0.562891  0.572271\n",
            "1    8780  0.409079    0.651576  0.696336  0.444856  0.550462\n",
            "2    8781  0.572895    0.649680  0.523324  0.502098  0.561999\n",
            "3   18644  0.376614    0.495247  0.533686  0.535156  0.485176\n",
            "\n",
            "R2 Scores:\n",
            "  Station       NO2  NOX as NO2      PM10     PM2.5\n",
            "0    8779 -0.078366   -0.005273 -0.026507  0.239915\n",
            "1    8780  0.325141    0.326838 -0.287271  0.286911\n",
            "2    8781  0.099753    0.073490 -0.116470  0.253212\n",
            "3   18644  0.378170    0.276115  0.164730  0.321316\n",
            "updating: content/data/ (stored 0%)\n",
            "updating: content/data/18644.csv (deflated 57%)\n",
            "updating: content/data/8781.csv (deflated 57%)\n",
            "updating: content/data/8780.csv (deflated 56%)\n",
            "updating: content/data/8779.csv (deflated 56%)\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}