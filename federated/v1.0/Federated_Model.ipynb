{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Federated_Model.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zQrq1O2Ite_q"
      },
      "source": [
        "## Federated Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LDuxGE-VtEg9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e3273924-926f-4369-f019-4a3c7c315162"
      },
      "source": [
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import missingno as msno\n",
        "import re\n",
        "import os\n",
        "import datetime\n",
        "import IPython\n",
        "import IPython.display\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "!pip install --quiet tensorflow_federated\n",
        "import tensorflow_federated as tff\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "mpl.rcParams['figure.figsize'] = (8, 6)\n",
        "mpl.rcParams['axes.grid'] = False"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[K     |████████████████████████████████| 522kB 8.8MB/s \n",
            "\u001b[K     |████████████████████████████████| 112kB 12.5MB/s \n",
            "\u001b[K     |████████████████████████████████| 3.0MB 15.8MB/s \n",
            "\u001b[K     |████████████████████████████████| 153kB 42.1MB/s \n",
            "\u001b[K     |████████████████████████████████| 174kB 43.1MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.1MB 45.7MB/s \n",
            "\u001b[?25h  Building wheel for absl-py (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: datascience 0.10.6 has requirement folium==0.2.1, but you'll have folium 0.8.3 which is incompatible.\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O2RZsSmyuKU5"
      },
      "source": [
        "## Read Air Pollutants Data\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cDv9r5PetnRQ"
      },
      "source": [
        "def create_df(PATH):\n",
        "    # Loading the data\n",
        "    df = pd.read_csv(PATH,sep=';',skiprows=12)\n",
        "    df = df.drop(columns=['Slut'])\n",
        "    column_indices = {i: name for i, name in enumerate(df.columns)}\n",
        "    # Renaming column names\n",
        "    for i in range(0,len(column_indices)):\n",
        "        column = column_indices[i]\n",
        "        if column.startswith('Black Carbon'):\n",
        "            df.rename(columns = lambda x: re.sub('Black Carbon.*','Black Carbon',x), inplace = True)\n",
        "        if column.startswith('CO'):\n",
        "            df.rename(columns = lambda x: re.sub('CO.*','CO',x), inplace = True)\n",
        "        if column.startswith('O3'):\n",
        "            df.rename(columns = lambda x: re.sub('O3.*','O3',x), inplace = True)\n",
        "        if column.startswith('NO2'):\n",
        "            df.rename(columns = lambda x: re.sub('NO2.*','NO2',x), inplace = True)\n",
        "        if column.startswith('NOX as NO2'):\n",
        "            df.rename(columns = lambda x: re.sub('NOX as NO2.*','NOX as NO2',x), inplace = True)\n",
        "        if column.startswith('PM10'):\n",
        "            df.rename(columns = lambda x: re.sub('PM10.*','PM10',x), inplace = True)\n",
        "        if column.startswith('PM2.5'):\n",
        "            df.rename(columns = lambda x: re.sub('PM2.5.*','PM2.5',x), inplace = True)\n",
        "    \n",
        "    return df\n",
        "\n",
        "PATH1 = '/content/shair-8779-1-6-3.csv'\n",
        "PATH2 = '/content/shair-8780-1-6-3.csv'\n",
        "PATH3 = '/content/shair-8781-1-6-1.csv'\n",
        "PATH4 = '/content/shair-18644-1-6-3.csv'\n",
        "station1 = create_df(PATH1)\n",
        "station2 = create_df(PATH2)\n",
        "station3 = create_df(PATH3)\n",
        "station4 = create_df(PATH4)\n",
        "smape_values = pd.DataFrame(columns=['Station','NO2','NOX as NO2','PM10','PM2.5','Average'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NgTtnbtVuP1B"
      },
      "source": [
        "## Impute Missing Values"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZN7FYkwmtuuA"
      },
      "source": [
        "# to print zero entries, negative entries and null entries\n",
        "def missingstats():  \n",
        "  print('Station 1 Missing Values Stats:')\n",
        "  print('----------------------------------------------')\n",
        "  print('\\nNo of Zeros Entries:\\n',(station1.select_dtypes(include=['float64']) == 0).astype(int).sum(axis=0))\n",
        "  print('\\nNo of Negative Entries:\\n',(station1.select_dtypes(include=['float64']) < 0).astype(int).sum(axis=0))\n",
        "  print('\\nNo of Null Entries:\\n',(station1.select_dtypes(include=['float64']).isnull()).astype(int).sum(axis=0))\n",
        "  print('\\n\\n')\n",
        "\n",
        "  print('Station 2 Missing Values Stats:')\n",
        "  print('----------------------------------------------')\n",
        "  print('\\nNo of Zeros Entries:\\n',(station2.select_dtypes(include=['float64']) == 0).astype(int).sum(axis=0))\n",
        "  print('\\nNo of Negative Entries:\\n',(station2.select_dtypes(include=['float64']) < 0).astype(int).sum(axis=0))\n",
        "  print('\\nNo of Null Entries:\\n',(station2.select_dtypes(include=['float64']).isnull()).astype(int).sum(axis=0))\n",
        "  print('\\n\\n')\n",
        "\n",
        "  print('Station 3 Missing Values Stats:')\n",
        "  print('----------------------------------------------')\n",
        "  print('\\nNo of Zeros Entries:\\n',(station3.select_dtypes(include=['float64']) == 0).astype(int).sum(axis=0))\n",
        "  print('\\nNo of Negative Entries:\\n',(station3.select_dtypes(include=['float64']) < 0).astype(int).sum(axis=0))\n",
        "  print('\\nNo of Null Entries:\\n',(station3.select_dtypes(include=['float64']).isnull()).astype(int).sum(axis=0))\n",
        "  print('\\n\\n')\n",
        "\n",
        "  print('Station 4 Missing Values Stats:')\n",
        "  print('----------------------------------------------')\n",
        "  print('\\nNo of Zeros Entries:\\n',(station4.select_dtypes(include=['float64']) == 0).astype(int).sum(axis=0))\n",
        "  print('\\nNo of Negative Entries:\\n',(station4.select_dtypes(include=['float64']) < 0).astype(int).sum(axis=0))\n",
        "  print('\\nNo of Null Entries:\\n',(station4.select_dtypes(include=['float64']).isnull()).astype(int).sum(axis=0))\n",
        "  print('\\n\\n')\n",
        "  return None\n",
        "  \n",
        "station1 = station1.set_index('Start')\n",
        "station1[station1 < 0] = 0\n",
        "station1.reset_index(drop=False, inplace= True)\n",
        "station1 = station1.interpolate(method ='linear', limit_direction ='forward')\n",
        "station1['Station'] = 1\n",
        "\n",
        "\n",
        "station2 = station2.set_index('Start')\n",
        "station2[station2 < 0] = 0\n",
        "station2.reset_index(drop=False, inplace= True)\n",
        "station2 = station2.interpolate(method ='linear', limit_direction ='forward')\n",
        "station2['Station'] = 2\n",
        "\n",
        "station3 = station3.set_index('Start')\n",
        "station3[station3 < 0] = 0\n",
        "station3.reset_index(drop=False, inplace= True)\n",
        "station3 = station3.interpolate(method ='linear', limit_direction ='forward')\n",
        "station3['Station'] = 3\n",
        "\n",
        "station4 = station4.set_index('Start')\n",
        "station4[station4 < 0] = 0\n",
        "station4.reset_index(drop=False, inplace= True)\n",
        "station4 = station4.interpolate(method ='linear', limit_direction ='forward')\n",
        "station4['Station'] = 4\n",
        "#missingstats()\n",
        "station = station1.append([station2, station3, station4])\n",
        "to_be_normalized_columns = ['NO2', 'NOX as NO2','PM2.5']\n",
        "\n",
        "target_column = \"PM10\"\n",
        "standard_scaler_x = StandardScaler(with_mean=True, with_std=True)\n",
        "station[to_be_normalized_columns + [target_column]] = standard_scaler_x.fit_transform(station[to_be_normalized_columns + [target_column]])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 406
        },
        "id": "RMUTzaGdSQ7f",
        "outputId": "ff36baf2-4016-4f38-a1aa-c37782884871"
      },
      "source": [
        "station"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Start</th>\n",
              "      <th>NO2</th>\n",
              "      <th>NOX as NO2</th>\n",
              "      <th>PM10</th>\n",
              "      <th>PM2.5</th>\n",
              "      <th>Station</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2015-01-01 00:00</td>\n",
              "      <td>-0.046829</td>\n",
              "      <td>0.192921</td>\n",
              "      <td>3.845831</td>\n",
              "      <td>6.227233</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2015-01-01 01:00</td>\n",
              "      <td>-0.329492</td>\n",
              "      <td>-0.298143</td>\n",
              "      <td>-0.466392</td>\n",
              "      <td>0.169195</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2015-01-01 02:00</td>\n",
              "      <td>-0.631784</td>\n",
              "      <td>-0.512280</td>\n",
              "      <td>-0.480977</td>\n",
              "      <td>0.154974</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2015-01-01 03:00</td>\n",
              "      <td>-0.643562</td>\n",
              "      <td>-0.386696</td>\n",
              "      <td>-0.286514</td>\n",
              "      <td>0.112312</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2015-01-01 04:00</td>\n",
              "      <td>-0.922299</td>\n",
              "      <td>-0.570241</td>\n",
              "      <td>-0.369161</td>\n",
              "      <td>-0.314311</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>43819</th>\n",
              "      <td>2019-12-31 19:00</td>\n",
              "      <td>-0.642353</td>\n",
              "      <td>-0.553591</td>\n",
              "      <td>-0.189561</td>\n",
              "      <td>-1.042729</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>43820</th>\n",
              "      <td>2019-12-31 20:00</td>\n",
              "      <td>-0.879216</td>\n",
              "      <td>-0.683478</td>\n",
              "      <td>-0.558982</td>\n",
              "      <td>-1.153335</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>43821</th>\n",
              "      <td>2019-12-31 21:00</td>\n",
              "      <td>-0.903386</td>\n",
              "      <td>-0.698859</td>\n",
              "      <td>-0.626622</td>\n",
              "      <td>-1.153335</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>43822</th>\n",
              "      <td>2019-12-31 22:00</td>\n",
              "      <td>-0.743866</td>\n",
              "      <td>-0.639043</td>\n",
              "      <td>-0.444514</td>\n",
              "      <td>-1.061163</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>43823</th>\n",
              "      <td>2019-12-31 23:00</td>\n",
              "      <td>-0.758368</td>\n",
              "      <td>-0.652715</td>\n",
              "      <td>-0.064686</td>\n",
              "      <td>-0.839952</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>175296 rows × 6 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                  Start       NO2  NOX as NO2      PM10     PM2.5  Station\n",
              "0      2015-01-01 00:00 -0.046829    0.192921  3.845831  6.227233        1\n",
              "1      2015-01-01 01:00 -0.329492   -0.298143 -0.466392  0.169195        1\n",
              "2      2015-01-01 02:00 -0.631784   -0.512280 -0.480977  0.154974        1\n",
              "3      2015-01-01 03:00 -0.643562   -0.386696 -0.286514  0.112312        1\n",
              "4      2015-01-01 04:00 -0.922299   -0.570241 -0.369161 -0.314311        1\n",
              "...                 ...       ...         ...       ...       ...      ...\n",
              "43819  2019-12-31 19:00 -0.642353   -0.553591 -0.189561 -1.042729        4\n",
              "43820  2019-12-31 20:00 -0.879216   -0.683478 -0.558982 -1.153335        4\n",
              "43821  2019-12-31 21:00 -0.903386   -0.698859 -0.626622 -1.153335        4\n",
              "43822  2019-12-31 22:00 -0.743866   -0.639043 -0.444514 -1.061163        4\n",
              "43823  2019-12-31 23:00 -0.758368   -0.652715 -0.064686 -0.839952        4\n",
              "\n",
              "[175296 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GMsmLj_nRHWD"
      },
      "source": [
        "import collections\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "NUM_EPOCHS = 5\n",
        "BATCH_SIZE = 20\n",
        "SHUFFLE_BUFFER = 100\n",
        "PREFETCH_BUFFER = 10\n",
        "\n",
        "def preprocess(dataset):\n",
        "\n",
        "  def batch_format_fn(element):\n",
        "      return collections.OrderedDict(x=element['x'], y=element['y'])\n",
        "\n",
        "  return dataset.repeat(NUM_EPOCHS).shuffle(SHUFFLE_BUFFER).batch(\n",
        "      BATCH_SIZE).map(batch_format_fn).prefetch(PREFETCH_BUFFER)\n",
        "\n",
        "\n",
        "def make_federated_data():\n",
        "  dfs = [x for _, x in station.groupby('Station')]\n",
        "  train_datasets = []\n",
        "  test_datasets = []\n",
        "  for df in dfs:\n",
        "    X = df.copy()\n",
        "    X.pop('Start')  \n",
        "    y = df[['PM10']]\n",
        "    X.pop('Station') \n",
        "    train_x = X.iloc[0:41592,:] \n",
        "    test_x = X.iloc[41592:,:]\n",
        "    train_y = y.iloc[0:41592,:]\n",
        "    test_y = y.iloc[41592:,:]\n",
        "    train_dataset = tf.data.Dataset.from_tensor_slices(\n",
        "                    ({'x': train_x.values, 'y': train_y.values}))\n",
        "        \n",
        "    test_dataset = tf.data.Dataset.from_tensor_slices(\n",
        "                    ({'x': test_x.values, 'y': test_y.values}))\n",
        "  \n",
        "    preprocessed_train_dataset = preprocess(train_dataset)\n",
        "    preprocessed_test_dataset = preprocess(test_dataset)\n",
        "\n",
        "    train_datasets.append(preprocessed_train_dataset)\n",
        "    test_datasets.append(preprocessed_test_dataset)\n",
        "        \n",
        "  return train_datasets, test_datasets"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Y4lKK_8178t"
      },
      "source": [
        "train_datasets, test_datasets = make_federated_data()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uyZFXhuyFoRn",
        "outputId": "2f8554b1-0953-4092-c774-125e4b37acdd"
      },
      "source": [
        "print(train_datasets[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<PrefetchDataset shapes: OrderedDict([(x, (None, 4)), (y, (None, 1))]), types: OrderedDict([(x, tf.float64), (y, tf.float64)])>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PkJpBsDSE7b9"
      },
      "source": [
        "def build_model():\n",
        "  model = keras.Sequential([\n",
        "    layers.Dense(64, activation='relu', input_shape=[4]),\n",
        "    layers.Dense(1)\n",
        "  ])\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I6NN0f4a38EJ"
      },
      "source": [
        "def create_tff_model():\n",
        "  return tff.learning.from_keras_model(build_model(), \n",
        "                                       input_spec=train_datasets[0].element_spec,\n",
        "                                       loss=tf.keras.losses.MeanAbsoluteError(),\n",
        "                                       metrics=[tf.keras.metrics.MeanAbsoluteError()])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KQ4chGLA3-6o",
        "outputId": "66a7b760-7e88-49a0-890b-b75e3ee8e2a3"
      },
      "source": [
        "print(\"Create averaging process\")\n",
        "iterative_process = tff.learning.build_federated_averaging_process(model_fn=create_tff_model,\n",
        "                                                                   client_optimizer_fn = lambda: tf.keras.optimizers.SGD(0.002))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Create averaging process\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hsnLJr4vGHpJ",
        "outputId": "4a460188-acb7-4cea-b1fd-bb379009ca66"
      },
      "source": [
        "!pip install nest_asyncio\n",
        "import nest_asyncio\n",
        "nest_asyncio.apply()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: nest_asyncio in /usr/local/lib/python3.6/dist-packages (1.4.3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4OorKFzv5OLc",
        "outputId": "f90cece6-1233-4a28-e6e9-7008637d2d2f"
      },
      "source": [
        "print(\"Initzialize averaging process\")\n",
        "state = iterative_process.initialize()\n",
        "\n",
        "print(\"Start iterations\")\n",
        "for _ in range(10):\n",
        "  state, metrics = iterative_process.next(state, train_datasets)\n",
        "  print('metrics={}'.format(metrics))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Initzialize averaging process\n",
            "Start iterations\n",
            "metrics=OrderedDict([('broadcast', ()), ('aggregation', OrderedDict([('value_sum_process', ()), ('weight_sum_process', ())])), ('train', OrderedDict([('mean_absolute_error', 0.024144776), ('loss', 0.024144776)]))])\n",
            "metrics=OrderedDict([('broadcast', ()), ('aggregation', OrderedDict([('value_sum_process', ()), ('weight_sum_process', ())])), ('train', OrderedDict([('mean_absolute_error', 0.006381416), ('loss', 0.006381416)]))])\n",
            "metrics=OrderedDict([('broadcast', ()), ('aggregation', OrderedDict([('value_sum_process', ()), ('weight_sum_process', ())])), ('train', OrderedDict([('mean_absolute_error', 0.004773584), ('loss', 0.004773584)]))])\n",
            "metrics=OrderedDict([('broadcast', ()), ('aggregation', OrderedDict([('value_sum_process', ()), ('weight_sum_process', ())])), ('train', OrderedDict([('mean_absolute_error', 0.0042899773), ('loss', 0.0042899773)]))])\n",
            "metrics=OrderedDict([('broadcast', ()), ('aggregation', OrderedDict([('value_sum_process', ()), ('weight_sum_process', ())])), ('train', OrderedDict([('mean_absolute_error', 0.0041631223), ('loss', 0.0041631223)]))])\n",
            "metrics=OrderedDict([('broadcast', ()), ('aggregation', OrderedDict([('value_sum_process', ()), ('weight_sum_process', ())])), ('train', OrderedDict([('mean_absolute_error', 0.0040988815), ('loss', 0.0040988815)]))])\n",
            "metrics=OrderedDict([('broadcast', ()), ('aggregation', OrderedDict([('value_sum_process', ()), ('weight_sum_process', ())])), ('train', OrderedDict([('mean_absolute_error', 0.004066186), ('loss', 0.004066186)]))])\n",
            "metrics=OrderedDict([('broadcast', ()), ('aggregation', OrderedDict([('value_sum_process', ()), ('weight_sum_process', ())])), ('train', OrderedDict([('mean_absolute_error', 0.004052349), ('loss', 0.004052349)]))])\n",
            "metrics=OrderedDict([('broadcast', ()), ('aggregation', OrderedDict([('value_sum_process', ()), ('weight_sum_process', ())])), ('train', OrderedDict([('mean_absolute_error', 0.0040234993), ('loss', 0.0040234993)]))])\n",
            "metrics=OrderedDict([('broadcast', ()), ('aggregation', OrderedDict([('value_sum_process', ()), ('weight_sum_process', ())])), ('train', OrderedDict([('mean_absolute_error', 0.0039882963), ('loss', 0.0039882963)]))])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PenxmGXIHIsO",
        "outputId": "7a0ac724-8b92-465a-ce06-296ad6812684"
      },
      "source": [
        "# Global model evaluated per individual client\n",
        "for i in range(len(test_datasets)):\n",
        "    test_metrics = evaluation(state.model, [test_datasets[i]])\n",
        "    print(test_metrics)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "OrderedDict([('mean_absolute_error', 0.0016565085), ('loss', 0.0016565085)])\n",
            "OrderedDict([('mean_absolute_error', 0.0016345803), ('loss', 0.0016345803)])\n",
            "OrderedDict([('mean_absolute_error', 0.0013538589), ('loss', 0.0013538589)])\n",
            "OrderedDict([('mean_absolute_error', 0.0015299591), ('loss', 0.0015299591)])\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}